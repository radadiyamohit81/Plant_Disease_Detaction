{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEGo98P62Mbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eBI0N0b2Kzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall -y Pillow\n",
        "# Install the new one\n",
        "!pip install Pillow==5.3.0\n",
        "!pip install image\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln5JQ4xm2Qgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaKtdvUw2bYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from torch.optim import lr_scheduler\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from os.path import exists\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "data_dir = '/PlantVillage'\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir = data_dir + '/val'\n",
        "nThreads = 4\n",
        "batch_size = 32\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "import json\n",
        "with open('categories.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahv1Dae92oq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "\n",
        "data_dir = 'PlantVillage'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "# Using the image datasets and the trainforms, define the dataloaders\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLEe_F2U2vbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Load resnet-152 pre-trained network\n",
        "model = models.resnet152(pretrained=True)\n",
        "# Freeze parameters so we don't backprop through them\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#Let's check the model architecture:\n",
        "    print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvyKazm-2wcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "# Creating the classifier ordered dictionary first\n",
        "\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(512, 39)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "\n",
        "# Replacing the pretrained model classifier with our classifier\n",
        "model.fc = classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whuybtja2zco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best valid accuracy: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUJXPbC022De",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 10\n",
        "if use_gpu:\n",
        "    print (\"Using GPU: \"+ str(use_gpu))\n",
        "    model = model.cuda()\n",
        "\n",
        "# NLLLoss because our output is LogSoftmax\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Adam optimizer with a learning rate\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "# Decay LR by a factor of 0.1 every 5 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n",
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teJKFi-G285X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, dataloaders, device):\n",
        "  model.eval()\n",
        "  accuracy = 0\n",
        "  \n",
        "  model.to(device)\n",
        "    \n",
        "  for images, labels in dataloaders['val']:\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "      \n",
        "    output = model.forward(images)\n",
        "    ps = torch.exp(output)\n",
        "    equality = (labels.data == ps.max(1)[1])\n",
        "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "      \n",
        "    print(\"Testing Accuracy: {:.3f}\".format(accuracy/len(dataloaders['val'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zcCwjHw29z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.class_to_idx = dataloaders['train'].dataset.class_to_idx\n",
        "model.epochs = num_epochs\n",
        "checkpoint = {'input_size': [3, 224, 224],\n",
        "                 'batch_size': dataloaders['train'].batch_size,\n",
        "                  'output_size': 39,\n",
        "                  'state_dict': model.state_dict(),\n",
        "                  'data_transforms': data_transforms,\n",
        "                  'optimizer_dict':optimizer.state_dict(),\n",
        "                  'class_to_idx': model.class_to_idx,\n",
        "                  'epoch': model.epochs}\n",
        "torch.save(checkpoint, 'plants9615_checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh9QK2gF3Asi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = models.resnet152()\n",
        "    \n",
        "    # Our input_size matches the in_features of pretrained model\n",
        "    input_size = 2048\n",
        "    output_size = 39\n",
        "    \n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(2048, 512)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          #('dropout1', nn.Dropout(p=0.2)),\n",
        "                          ('fc2', nn.Linear(512, 39)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "\n",
        "# Replacing the pretrained model classifier with our classifier\n",
        "    model.fc = classifier\n",
        "    \n",
        "    \n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    return model, checkpoint['class_to_idx']\n",
        "\n",
        "# Get index to class mapping\n",
        "loaded_model, class_to_idx = load_checkpoint('plants9615_checkpoint.pth')\n",
        "idx_to_class = { v : k for k,v in class_to_idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHnbNYMH3C-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_image(image):\n",
        "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "        returns an Numpy array\n",
        "    '''\n",
        "    \n",
        "    # Process a PIL image for use in a PyTorch model\n",
        "\n",
        "    size = 256, 256\n",
        "    image.thumbnail(size, Image.ANTIALIAS)\n",
        "    image = image.crop((128 - 112, 128 - 112, 128 + 112, 128 + 112))\n",
        "    npImage = np.array(image)\n",
        "    npImage = npImage/255.\n",
        "        \n",
        "    imgA = npImage[:,:,0]\n",
        "    imgB = npImage[:,:,1]\n",
        "    imgC = npImage[:,:,2]\n",
        "    \n",
        "    imgA = (imgA - 0.485)/(0.229) \n",
        "    imgB = (imgB - 0.456)/(0.224)\n",
        "    imgC = (imgC - 0.406)/(0.225)\n",
        "        \n",
        "    npImage[:,:,0] = imgA\n",
        "    npImage[:,:,1] = imgB\n",
        "    npImage[:,:,2] = imgC\n",
        "    \n",
        "    npImage = np.transpose(npImage, (2,0,1))\n",
        "    \n",
        "    return npImage\n",
        "\n",
        "def imshow(image, ax=None, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # PyTorch tensors assume the color channel is the first dimension\n",
        "    # but matplotlib assumes is the third dimension\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    # Undo preprocessing\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    \n",
        "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
        "    image = np.clip(image, 0, 1)\n",
        "    \n",
        "    ax.imshow(image)\n",
        "    \n",
        "    return ax\n",
        "\n",
        "def predict(image_path, model, topk=5):\n",
        "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
        "    '''\n",
        "    \n",
        "    # Implement the code to predict the class from an image file\n",
        "    \n",
        "    image = torch.FloatTensor([process_image(Image.open(image_path))])\n",
        "    model.eval()\n",
        "    output = model.forward(Variable(image))\n",
        "    pobabilities = torch.exp(output).data.numpy()[0]\n",
        "    \n",
        "\n",
        "    top_idx = np.argsort(pobabilities)[-topk:][::-1] \n",
        "    top_class = [idx_to_class[x] for x in top_idx]\n",
        "    top_probability = pobabilities[top_idx]\n",
        "\n",
        "    return top_probability, top_class\n",
        "\n",
        "print (predict('PlantVillage/val/Tomato___Septoria_leaf_spot/046a86f0-bdae-438b-9c1f-b840c706f409___JR_Sept.L.S 8530.JPG', loaded_model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iNsVCWQ3FQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def view_classify(img, probabilities, classes, mapper):\n",
        "    ''' Function for viewing an image and it's predicted classes.\n",
        "    '''\n",
        "    img_filename = img.split('/')[-2]\n",
        "    img = Image.open(img)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,10), ncols=1, nrows=2)\n",
        "    flower_name = mapper[img_filename]\n",
        "    \n",
        "    ax1.set_title(flower_name)\n",
        "    ax1.imshow(img)\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    y_pos = np.arange(len(probabilities))\n",
        "    ax2.barh(y_pos, probabilities)\n",
        "    ax2.set_yticks(y_pos)\n",
        "    ax2.set_yticklabels([mapper[x] for x in classes])\n",
        "    ax2.invert_yaxis()\n",
        "\n",
        "img = 'PlantVillage/val/Tomato___Tomato_Yellow_Leaf_Curl_Virus/124d64c3-37d2-4192-b7ec-998a811b4e1e___UF.GRC_YLCV_Lab 01626.JPG'\n",
        "\n",
        "p, c = predict(img, loaded_model)\n",
        "view_classify(img, p, c, cat_to_name)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}